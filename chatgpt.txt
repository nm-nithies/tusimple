It should be possible to fold the BatchNormalization operation in both Conv operations even if there is a Relu in between for the cases in scaledYoloV4.onnx.

BatchNormalization is doing

y = (x - input_mean) / sqrt(input_var + epsilon) * scale + B
Or
y = x * (scale / sqrt(input_var + epsilon)) - (input_mean - B)
If both 'scale / sqrt(input_var + epsilon)' and 'input_mean - B' are positive the first Relu can be removed, ie Conv+Relu+BatchNorm+Relu is the same as Conv+BatchNorm+Relu in that case

Fot the usecase in this graph this will always be the case, ie

sqrt(input_var + epsilon) is always positive
input_mean will always be positive because it was calculated based on tensors with only positive values (after Relu)
scale is not used (always 1) so 'scale / sqrt(input_var + epsilon)' will always be psoitive
B is not used (always 0) so 'input_mean - B' will always be positive
So if 'scale / sqrt(input_var + epsilon)' and 'input_mean - B' are positive the first Relu can be removed and the BatchNormalization can be folded into the coefficients of the Conv operation
