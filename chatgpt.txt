 def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            # check if the module has set the attribute `fp16_enabled`, if not,
            # just fallback to the original method.
            if not isinstance(args[0], supported_types):
                raise TypeError('@auto_fp16 can only be used to decorate the '
                                f'method of those classes {supported_types}')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)



{'rescale': True, 'img_metas': [[{'flip': False, 'pcd_horizontal_flip': False, 'pcd_vertical_flip': False, 'box_mode_3d': <Box3DMode.LIDAR: 0>, 'box_type_3d': <class 'mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes'>, 'sample_idx': '30e55a3ec6184d8cb1944b39ba19d622', 'pcd_scale_factor': 1.0, 'pts_filename': 'data/nuscenes/samples/LIDAR_TOP/n015-2018-07-11-11-54-16+0800__LIDAR_TOP__1531281439800013.pcd.bin'}]]



{
    'rescale': True,
    'img_metas': [
        [
            {
                'flip': False,
                'pcd_horizontal_flip': False,
                'pcd_vertical_flip': False,
                'box_mode_3d': Box3DMode.LIDAR,
                'box_type_3d': mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes,
                'sample_idx': '30e55a3ec6184d8cb1944b39ba19d622',
                'pcd_scale_factor': 1.0,
                'pts_filename': 'data/nuscenes/samples/LIDAR_TOP/n015-2018-07-11-11-54-16+0800__LIDAR_TOP__1531281439800013.pcd.bin'
            }
        ]
    ]
}


  File "/usr/local/lib/python3.8/dist-packages/mmcv/runner/fp16_utils.py", line 124, in new_func
    return old_func(*args,**kwargs)
  File "/workspace/flashocc/FlashOCC/mmdetection3d/mmdet3d/models/detectors/base.py", line 74, in forward
    return self.forward_test(**kwargs)
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/detectors/bevdet.py", line 216, in forward_test
    return self.simple_test(points[0], img_metas[0], img_inputs[0],
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/detectors/bevdet_occ.py", line 168, in simple_test
    img_feats, _, _ = self.extract_feat(
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/detectors/bevdet.py", line 126, in extract_feat
    img_feats, depth = self.extract_img_feat(img_inputs, img_metas, **kwargs)
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/detectors/bevdet.py", line 106, in extract_img_feat
    x, depth = self.img_view_transformer([x] + img_inputs[1:7])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1106, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1094, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py", line 431, in forward
    return self.view_transform(input, depth, tran_feat)
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py", line 404, in view_transform
    return self.view_transform_core(input, depth, tran_feat)
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py", line 380, in view_transform_core
    bev_feat = self.voxel_pooling_v2(
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py", line 261, in voxel_pooling_v2
    bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/ops/bev_pool_v2/bev_pool.py", line 102, in bev_pool_v2
    x = QuickCumsumCuda.apply(depth, feat, ranks_depth, ranks_feat, ranks_bev,
  File "/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/ops/bev_pool_v2/bev_pool.py", line 29, in forward
    bev_pool_v2_ext.bev_pool_v2_forward(
RuntimeError: t == DeviceType::CUDAINTERNAL ASSERT FAILED at "/usr/local/lib/python3.8/dist-packages/torch/include/c10/cuda/impl/CUDAGuardImpl.h":24, please report a bug to PyTorch. 




         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]]],
       device='cuda:0'), tensor([[[[1.2726e+03, 0.0000e+00, 8.2662e+02],
          [0.0000e+00, 1.2726e+03, 4.7975e+02],
          [0.0000e+00, 0.0000e+00, 1.0000e+00]],

         [[1.2664e+03, 0.0000e+00, 8.1627e+02],
          [0.0000e+00, 1.2664e+03, 4.9151e+02],
          [0.0000e+00, 0.0000e+00, 1.0000e+00]],

         [[1.2608e+03, 0.0000e+00, 8.0797e+02],
          [0.0000e+00, 1.2608e+03, 4.9533e+02],
          [0.0000e+00, 0.0000e+00, 1.0000e+00]],

         [[1.2567e+03, 0.0000e+00, 7.9211e+02],
          [0.0000e+00, 1.2567e+03, 4.9278e+02],
          [0.0000e+00, 0.0000e+00, 1.0000e+00]],

         [[8.0922e+02, 0.0000e+00, 8.2922e+02],
          [0.0000e+00, 8.0922e+02, 4.8178e+02],
          [0.0000e+00, 0.0000e+00, 1.0000e+00]],

         [[1.2595e+03, 0.0000e+00, 8.0725e+02],
          [0.0000e+00, 1.2595e+03, 5.0120e+02],
          [0.0000e+00, 0.0000e+00, 1.0000e+00]]]], device='cuda:0'), tensor([[[[0.4400, 0.0000, 0.0000],
          [0.0000, 0.4400, 0.0000],
          [0.0000, 0.0000, 1.0000]],

         [[0.4400, 0.0000, 0.0000],
          [0.0000, 0.4400, 0.0000],
          [0.0000, 0.0000, 1.0000]],

         [[0.4400, 0.0000, 0.0000],
          [0.0000, 0.4400, 0.0000],
          [0.0000, 0.0000, 1.0000]],

         [[0.4400, 0.0000, 0.0000],
          [0.0000, 0.4400, 0.0000],
          [0.0000, 0.0000, 1.0000]],

         [[0.4400, 0.0000, 0.0000],
          [0.0000, 0.4400, 0.0000],
          [0.0000, 0.0000, 1.0000]],

         [[0.4400, 0.0000, 0.0000],
          [0.0000, 0.4400, 0.0000],
          [0.0000, 0.0000, 1.0000]]]], device='cuda:0'), tensor([[[   0., -140.,    0.],
         [   0., -140.,    0.],
         [   0., -140.,    0.],
         [   0., -140.,    0.],
         [   0., -140.,    0.],
         [   0., -140.,    0.]]], device='cuda:0'), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]], device='cuda:0')]]}
False
/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:363: UserWarning: Skipping _decide_input_format
 -1
  warnings.warn("Skipping _decide_input_format\n {}".format(e.args[0]))
nms in trace fisrt
if condition
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:288: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
  ranks_depth = torch.range(
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:291: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  0, num_points // D - 1, dtype=torch.int, device=coor.device)   # [0, 1, ...,B*N*fH*fW-1]
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:290: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
  ranks_feat = torch.range(
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:301: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
  batch_idx = torch.range(0, B - 1).reshape(B, 1). \
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:302: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  expand(B, num_points // B).reshape(num_points, 1).to(coor)
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:309: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.
  if len(kept) == 0:
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:330: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.
  if len(interval_starts) == 0:
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:258: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/necks/view_transformer.py:259: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  int(self.grid_size[1]), int(self.grid_size[0]),
IN BEVPOOL
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
In Conv
/workspace/flashocc/FlashOCC/projects/mmdet3d_plugin/models/dense_heads/bev_occ_head.py:288: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  occ_res = occ_res.cpu().numpy().astype(np.uint8)     # (B, Dx, Dy, Dz)
Traceback (most recent call last):
  File "tools/test.py", line 359, in <module>
    main()
  File "tools/test.py", line 317, in main
    torch.onnx.export(
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/__init__.py", line 316, in export
    return utils.export(model, args, f, export_params, verbose, training,
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 107, in export
    _export(model, args, f, export_params, verbose, training, input_names, output_names,
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 724, in _export
    _model_to_graph(model, args, verbose, input_names,
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 493, in _model_to_graph
    graph, params, torch_out, module = _create_jit_graph(model, args)
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 437, in _create_jit_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args)
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 388, in _trace_and_get_graph_from_model
    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)
  File "/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py", line 1173, in _get_trace_graph
    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args_tuple, **args)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1106, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py", line 130, in forward
    graph, out = torch._C._create_graph_by_tracing(
RuntimeError: output 1 (14
[ CPULongType{} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.


Traceback (most recent call last):
  File "tools/test.py", line 359, in <module>
    main()
  File "tools/test.py", line 317, in main
    torch.onnx.export(
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/__init__.py", line 316, in export
    return utils.export(model, args, f, export_params, verbose, training,
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 107, in export
    _export(model, args, f, export_params, verbose, training, input_names, output_names,
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 724, in _export
    _model_to_graph(model, args, verbose, input_names,
  File "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py", line 519, in _model_to_graph
    assert len(params) + len(flatten_args) == sum(1 for _ in graph.inputs())
AssertionError

