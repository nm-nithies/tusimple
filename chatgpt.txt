# LayerNorm Discrepancy Analysis in MLP-Mixer Model

## Overview
This document outlines the analysis and solution for a significant output difference observed when fusing a LayerNorm pattern in an MLP-Mixer model to the ONNX LayerNorm operation. The issue was identified during output verification using ONNX Runtime.

## Observed Issue
- The MLP-Mixer model initially contained a LayerNorm pattern, which was later fused to an ONNX LayerNorm OP.
- When verifying the output using ONNX Runtime, a large discrepancy was observed between the two versions of the model.

## Root Cause Analysis
To identify the cause of the discrepancy, the following steps were undertaken:

### 1. Isolated Subgraph Verification
- A LayerNorm pattern subgraph was sliced and compared with the standalone ONNX LayerNorm OP.
- The outputs from both computations were found to be identical.

### 2. Reduction Mean Axis and Epsilon Verification
- The reduction axis used for computing the mean and variance was validated and matched in both implementations.
- The epsilon value (used for numerical stability) was confirmed to be the same in both cases.

### 3. Scale and Bias Range Check
- The scale and bias parameters were analyzed and confirmed to lie within the expected range (0 to 1).

### 4. MatMul Placement
- The LayerNorm operation is followed by a MatMul operation in the model.
- The subgraph containing **LayerNorm pattern + MatMul** and **LayerNorm OP + MatMul** was tested separately, and the outputs were found to be identical.
- Despite this, a large difference was observed when running the entire model with the fused LayerNorm OP.

### 5. Verification with Other Models in NNSDK
- The LayerNorm pattern was also verified against other models containing LayerNorm patterns in NNSDK.
- The results were consistent, confirming that the isolated LayerNorm behaves correctly, reinforcing that the issue arises when running the full model.

## Possible Solutions
- **Precision Loss Investigation:**
  - Verify if there is any precision loss introduced during the fusion process by analyzing floating-point precision in ONNX.
- **Execution Order Differences:**
  - The fused LayerNorm OP might be executed differently within the entire model, leading to numerical differences.
- **Numerical Stability Checks:**
  - Examine if numerical stability issues arise due to small differences in intermediate computations that amplify during later operations.
- **Alternative Fusion Approach:**
  - If fusion is necessary, consider an alternative method that ensures numerical consistency between the fused and non-fused implementations.

## Conclusion
The issue seems to stem from execution differences when running the full model, rather than an isolated LayerNorm discrepancy. Further debugging is required to analyze how the fused LayerNorm OP interacts within the complete graph.

## Next Steps
Continue refining the debugging process by analyzing intermediate activations throughout the full model execution and testing alternative fusion strategies to mitigate the discrepancy.
