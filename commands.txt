#include "conversion/mlir_translator.h"
#include "conversion/be_mlir_translator.h"
#include "llvm/ADT/bit.h"
#include "mli_types.hpp"
#include "mlir/IR/MLIRContext.h"
#include "mlir/IR/OwningOpRef.h"
#include "onnx/onnx_pb.h"
#include "operations_mli.hpp"
#include "src/Builder/FrontendDialectTransformer.hpp"
#include "src/Compiler/CompilerOptions.hpp"
#include "src/Compiler/CompilerUtils.hpp"

namespace nnmlir {
void ProcessMLIROpts(int argc, char** argv) {
    mlir::registerAsmPrinterCLOptions();
    mlir::registerMLIRContextCLOptions();
    mlir::registerPassManagerCLOptions();
    mlir::registerDefaultTimingManagerCLOptions();
    mlir::registerAsmPrinterCLOptions();
    onnx_mlir::removeUnrelatedOptions(
        {&onnx_mlir::OnnxMlirCommonOptions, &onnx_mlir::OnnxMlirOptions});
    if (!onnx_mlir::parseCustomEnvFlagsCommandLineOption(argc, argv,
                                                         &llvm::errs()) ||
        !llvm::cl::ParseCommandLineOptions(argc, argv))
        llvm::errs() << "Failed to parse options\n";
    onnx_mlir::initCompilerConfig();
}

void do_import(const onnx::ModelProto& model_proto, mlir::MLIRContext& context,
               mlir::OwningOpRef<mlir::ModuleOp>& module) {
    onnx_mlir::ImportOptions opts;
    opts.useOnnxModelTypes = true;
    opts.preserveMainGraphName = true;
    onnx_mlir::ImportFrontendModel(model_proto, context, module, opts);
}

void translateToMLIR(mlir::MLIRContext& context,
                     mlir::OwningOpRef<mlir::ModuleOp>& module,
                     const BackendGraph& graph) {
    onnx_mlir::loadDialects(context);
    onnx::ModelProto model_proto;
    from_be_graph(graph, model_proto);
    do_import(model_proto, context, module);
}

void translateToMLIR(mlir::MLIRContext& context,
                     mlir::OwningOpRef<mlir::ModuleOp>& module,
                     const Converter& converter) {
    onnx_mlir::loadDialects(context);
    do_import(converter.model_proto, context, module);
}

// Run the ONNX-MLIR NNAC pipeline to produce MLI serialized in an ONNX
// protobuf file. Then parse that protobuf file into the BackendGraph owned
// by the Converter object.
void runOnnxPipeline(Converter& converter, const std::string& inputFile) {
    // Set options for onnx-mlir compiler
    char onnxMLIFile[] = "/tmp/MLIProtoXXXXXX";
    int fd = mkstemp(onnxMLIFile);
    close(fd);
    onnx_mlir::preserveMainGraphName = true;
    onnx_mlir::histogramFile = "fake-histogram";
    onnx_mlir::outputOnnxFile = onnxMLIFile;
    onnx_mlir::useOnnxModelTypes = true;
    onnx_mlir::maccel.push_back(onnx_mlir::accel::Accelerator::Kind::NNAC);

    // Compile the input Protobuf
    mlir::MLIRContext context;
    onnx_mlir::loadDialects(context);
    mlir::OwningOpRef<mlir::ModuleOp> module;
    std::string errorMessage;
    int rc =
        onnx_mlir::processInputFile(inputFile, context, module, &errorMessage);
    if (rc != 0) {
        if (!errorMessage.empty()) llvm::errs() << errorMessage << "\n";
        return;
    }
    rc = compileModule(module, context, onnxMLIFile,
                       onnx_mlir::EmissionTargetType::EmitMLIProto);
    if (rc != 0) {
        llvm::errs()
            << "Unable to compile the MLIR module produced from input graph\n";
    }
    // Parse the output MLI Protobuf file
    converter.ParseModel(onnxMLIFile);

    // If successful, onnx-mlir will create 2 files (the MLI Protobuf and the
    // actual MLIR text file). Remove both.
    std::string MLIRFile = onnxMLIFile;
    MLIRFile += ".mli.mlir";
    unlink(onnxMLIFile);
    unlink(MLIRFile.c_str());

    // FIXME: a hack to add scales and zero-points to Values.
    for (auto* node : converter.backend_graph.OrderedNodes()) {
        auto inputs = converter.backend_graph.FindInputs(node->id);
        auto outputs = converter.backend_graph.FindOutputs(node->id);
        // FIXME: another hack - get rid of the ONNX Flatten nodes that
        // onnx-mlir leaves behind.
        if (node->operation.type == OperationType::MWNN_Flatten) {
            node->operation.type = OperationType::MAL_Flatten;
        }
        for (auto* input : inputs) {
            Tensor& tensor = input->tensor;
            if (tensor.shape.GetRank() <= 1 ||
                (tensor.shape.GetLayout() != XopLayout::BHWC &&
                 tensor.shape.GetLayout() != XopLayout::OIHW &&
                 tensor.shape.GetLayout() != XopLayout::BC &&
                 tensor.shape.GetLayout() != XopLayout::IO))
                continue;
            // Populate fake scales and zero_points vectors.
            // Assume that we are quantizing the channel and that channel
            // is the last dimension.
            unsigned quant_size =
                (tensor.shape.GetLayout() == XopLayout::OIHW ||
                 tensor.shape.GetLayout() == XopLayout::IO)
                    ? tensor.shape.GetO()
                    : tensor.shape.GetC();
            std::vector<float> scales(quant_size, 3.32e-3);
            std::vector<int> zero_points(quant_size, -128);
            TensorQuantizationAffine quant{{scales}, {zero_points}};
            input->tensor.quantization = quant;
            input->tensor.quantization.quantizedDimension =
                input->tensor.shape.GetRank() - 1;
        }
        for (auto* output : outputs) {
            Tensor& tensor = output->tensor;
            if (tensor.shape.GetRank() <= 1 ||
                (tensor.shape.GetLayout() != XopLayout::BHWC &&
                 tensor.shape.GetLayout() != XopLayout::OIHW &&
                 tensor.shape.GetLayout() != XopLayout::BC &&
                 tensor.shape.GetLayout() != XopLayout::IO))
                continue;
            unsigned quant_size =
                (tensor.shape.GetLayout() == XopLayout::OIHW ||
                 tensor.shape.GetLayout() == XopLayout::IO)
                    ? tensor.shape.GetO()
                    : tensor.shape.GetC();
            // Populate fake scales and zero_points vectors.
            // Assume that we are quantizing the channel and that channel
            // is the last dimension.
            std::vector<float> scales(quant_size, 3.32e-3);
            std::vector<int> zero_points(quant_size, -128);
            TensorQuantizationAffine quant{{scales}, {zero_points}};
            output->tensor.quantization = quant;
            output->tensor.quantization.quantizedDimension =
                output->tensor.shape.GetRank() - 1;
        }
    }
}

void test_mlir_conversion(Converter& converter, bool printAll) {
    // Produce initial MLIR from the input Protobuf.
    mlir::MLIRContext context;
    mlir::OwningOpRef<mlir::ModuleOp> module;
    translateToMLIR(context, module, converter);
    mlir::OpPrintingFlags flags;
    flags.enableDebugInfo();
    onnx_mlir::loadDialects(context);

    // Print the incoming MLIR module.
    llvm::outs() << "MLIR Module produced by onnx-mlir from input Protobuf\n";
    module->print(llvm::outs(), flags);

    if (!printAll) return;

    // Produce the Protobuf from the BE graph.
    onnx::ModelProto model_proto;
    snps_arc::metaware::nnac::backend::from_be_graph(converter, model_proto);

    // Print the Protobuf file if requested.
    llvm::outs() << "Contents of the incoming Protobuf:\n";
    llvm::outs() << converter.model_proto.DebugString();
    llvm::outs() << "Contents of the reconstructed Protobuf:\n";
    llvm::outs() << model_proto.DebugString();

    // Produce MLIR from the reconstructed Protobuf.
    do_import(model_proto, context, module);
    llvm::outs() << "MLIR Module produced from reconstructed Protobuf\n";
    module->print(llvm::outs(), flags);
}
}  // namespace nnmlir

namespace snps_arc::metaware::nnac::backend {
#define SET_ATTR_FROM_PROTO_SCAL_IF(Name, ProtoAccessor) \
    if (attr_name == #Name) Name = attr_proto.ProtoAccessor();
#define SET_ATTR_FROM_PROTO_SCAL_ELIF(Name, ProtoAccessor) \
    else if (attr_name == #Name) Name = attr_proto.ProtoAccessor();
#define SET_ATTR_FROM_PROTO_ARRAY_IF(Name, ProtoAccessor) \
    if (attr_name == #Name) {                             \
        for (auto entry : attr_proto.ProtoAccessor()) {   \
            Name.push_back(entry);                        \
        }                                                 \
    }
#define SET_ATTR_FROM_PROTO_ARRAY_ELIF(Name, ProtoAccessor) \
    else if (attr_name == #Name) {                          \
        for (auto entry : attr_proto.ProtoAccessor()) {     \
            Name.push_back(entry);                          \
        }                                                   \
    }

FPCPAttrAdapter::FPCPAttrAdapter(const mli::FusedPreluClipPoolingAttributes& in)
    : relu_type((int8_t)in.relu_type) {
    // Set MaxPool attributes if they're set on input.
    if (in.maxpool_attrs.has_value()) {
        const mwnn::MaxPoolAttributes& mpa = in.maxpool_attrs.value();
        pool_type = lib_mli::PoolingType::kMaxPooling;
        auto_pad = mpa.auto_pad;
        for (int32_t entry : mpa.kernel_shape) kernel_shape.push_back(entry);
        for (int32_t entry : mpa.strides) strides.push_back(entry);
        for (int32_t entry : mpa.dilations) dilations.push_back(entry);
        // The padding begin/end values in mli::FPCPAtributes are in a
        // single array with the first N/2 values being the begin values and
        // the last N/2 values being the end values.
        int half_way = mpa.pads.size() / 2;
        for (int idx = 0; idx < half_way; idx++)
            pad_begin.push_back(mpa.pads[idx]);
        for (int idx = half_way; idx < half_way * 2; idx++)
            pad_end.push_back(mpa.pads[idx]);
        ceil_mode = (bool)mpa.ceil_mode;
        col_storage = (bool)mpa.storage_order;
    }
    // FIXME: We are just using the raw bits from pos/neg_scale without
    // considering what they represent (from scale_mode). This is probably
    // incorrect behaviour and we'll need to fix it.
    for (const auto& entry : in.prelu_attr.pos_scale)
        pos_scale.push_back(entry.float_value);
    for (const auto& entry : in.prelu_attr.neg_scale)
        neg_scale.push_back(entry.float_value);
    for (const auto& entry : in.slopes) slopes.push_back(entry);

    for (auto entry : in.input1_pre_scale_bias.input_bias)
        input_bias.push_back(entry.int_value);
    for (int32_t entry : in.output_bias) output_bias.push_back(entry);
    for (const auto& entry : in.input1_pre_scale_bias.pre_scale) {
        pre_scale.push_back(entry.scale);
        pre_shift.push_back(entry.shift);
    }
    for (auto entry : in.min_val) min_val.push_back(entry.int_value);
    for (auto entry : in.max_val) max_val.push_back(entry.int_value);
    if (in.min_onnx.has_value()) min_onnx = in.min_onnx;
    if (in.max_onnx.has_value()) max_onnx = in.max_onnx;
}

FPCPAttrAdapter::FPCPAttrAdapter(const onnx::NodeProto& node_proto)
    : relu_type(0) {
    for (auto& attr_proto : node_proto.attribute()) {
        const std::string& attr_name = attr_proto.name();
        SET_ATTR_FROM_PROTO_ARRAY_IF(pre_scale, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pre_shift, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(input_bias, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(output_bias, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pos_scale, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(neg_scale, ints)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(relu_type, i)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(slopes, floats)
        else if (attr_name == "kernel_size_reciprocal") kernel_size_reciprocal =
            llvm::bit_cast<double>(attr_proto.i());
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(min_val, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(max_val, ints)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(min_onnx, f)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(max_onnx, f)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(auto_pad, s)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(ceil_mode, i)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(col_storage, i)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(dilations, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(kernel_shape, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pad_begin, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pad_end, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(strides, ints)
        else if (attr_name == "pool_type" && attr_proto.s() == "maxpool")
            pool_type = lib_mli::PoolingType::kMaxPooling;
    }
}

void FPCPAttrAdapter::to_fpcp_attrs(
    mli::FusedPreluClipPoolingAttributes& to_set) {
    to_set.relu_type = (mli::MLIReluType)relu_type;
    to_set.slopes = slopes;
    // FIXME: we need to know how to determine this.
    to_set.input_bias_dtype = lib_mli::MliElemType::MliElemTypeFp32;
    std::vector<mli::FixedPointScale> scale_shift;

    // The pre_scale and pre_shift are separate arrays in the MLIR attributes
    // but are part of the same array in mli. Ensure they're the same size,
    // create the FixedPointScale objects and add them piece-wise to the vector.
    assert(pre_scale.size() == pre_shift.size() &&
           "Expecting pre_scale and pre_shift to have the same size");
    for (auto entry : pre_scale) {
        mli::FixedPointScale tmp;
        tmp.scale = entry;
        to_set.input1_pre_scale_bias.pre_scale.push_back(tmp);
    }
    int idx = 0;
    for (auto entry : pre_shift) {
        to_set.input1_pre_scale_bias.pre_scale[idx++].shift = entry;
    }
    for (auto entry : input_bias) {
        mli::BiasType b;
        b.int_value = entry;
        to_set.input1_pre_scale_bias.input_bias.push_back(b);
    }

    // FIXME: once the FPCP MLIR ops get the scale_mode attribute, set it
    // accordingly. For now, assume that the scaling mode is f32 always.
    to_set.prelu_attr.scale_mode = lib_mli::MliElemType::MliElemTypeFp32;
    for (auto entry : pos_scale) {
        mli::DownScale tmp;
        tmp.float_value = entry;
        to_set.prelu_attr.pos_scale.push_back(tmp);
    }
    for (auto entry : neg_scale) {
        mli::DownScale tmp;
        tmp.float_value = entry;
        to_set.prelu_attr.neg_scale.push_back(tmp);
    }

    for (auto entry : output_bias) to_set.output_bias.push_back(entry);
    for (auto entry : min_val) to_set.min_val.push_back(entry);
    for (auto entry : max_val) to_set.max_val.push_back(entry);
    if (min_onnx.has_value()) to_set.min_onnx = min_onnx.value();
    if (max_onnx.has_value()) to_set.max_onnx = max_onnx.value();
    if (kernel_size_reciprocal.has_value())
        to_set.kernel_size_reciprocal = kernel_size_reciprocal.value();

    // We set the optional MaxPool attributes last.
    if (pool_type != lib_mli::PoolingType::kMaxPooling) return;
    if (auto_pad.has_value()) to_set.maxpool_attrs->auto_pad = auto_pad.value();
    if (ceil_mode.has_value())
        to_set.maxpool_attrs->ceil_mode = ceil_mode.value();
    if (!dilations.empty()) {
        to_set.maxpool_attrs->dilations.reserve(dilations.size());
        std::copy(dilations.begin(), dilations.end(),
                  to_set.maxpool_attrs->dilations.begin());
    }
    if (!kernel_shape.empty()) {
        to_set.maxpool_attrs->kernel_shape.reserve(kernel_shape.size());
        std::copy(kernel_shape.begin(), kernel_shape.end(),
                  to_set.maxpool_attrs->kernel_shape.begin());
    }
    if (!pad_begin.empty()) {
        to_set.maxpool_attrs->pads.reserve(pad_begin.size() * 2);
        std::copy(pad_begin.begin(), pad_begin.end(),
                  to_set.maxpool_attrs->pads.begin());
    }
    if (!pad_end.empty()) {
        assert(to_set.maxpool_attrs->pads.size() > 0 &&
               "Not expecting pad_end without pad_begin");
        std::copy(pad_end.begin(), pad_end.end(),
                  to_set.maxpool_attrs->pads.end());
    }
    if (col_storage.has_value())
        to_set.maxpool_attrs->storage_order = col_storage.value();
    if (!strides.empty()) {
        to_set.maxpool_attrs->strides.reserve(strides.size());
        std::copy(strides.begin(), strides.end(),
                  to_set.maxpool_attrs->strides.begin());
    }
}

FEPCPAttrAdapter::FEPCPAttrAdapter(
    const mli::FusedEltwisePreluClipPoolingAttributes& in)
    : FPCPAttrAdapter(in) {

    eltwise_op = in.eltwise_type;
    for (auto entry : in.input2_pre_scale_bias.input_bias)
        input_bias2.push_back(entry.int_value);
    for (const auto& entry : in.input2_pre_scale_bias.pre_scale) {
        pre_scale2.push_back(entry.scale);
        pre_shift2.push_back(entry.shift);
    }
}

FEPCPAttrAdapter::FEPCPAttrAdapter(const onnx::NodeProto& node_proto)
    : FPCPAttrAdapter(node_proto) {
    for (auto& attr_proto : node_proto.attribute()) {
        const std::string& attr_name = attr_proto.name();
        SET_ATTR_FROM_PROTO_ARRAY_IF(pre_scale2, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pre_shift2, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(input_bias2, ints)
        else if (attr_name == "eltwise_op") eltwise_op =
            (mli::EltwiseType)attr_proto.i();
    }
}

void FEPCPAttrAdapter::to_fepcp_attrs(
    mli::FusedEltwisePreluClipPoolingAttributes& to_set) {
    to_fpcp_attrs(to_set);
    to_set.eltwise_type = eltwise_op;

    // Same requirements apply to pre_scale2/pre_shift2 as above for
    // pre_scale/pre_shift.
    assert(pre_scale2.size() == pre_shift2.size() &&
           "Expecting pre_scale2 and pre_shift2 to have the same size");
    for (auto entry : pre_scale2) {
        mli::FixedPointScale tmp;
        tmp.scale = entry;
        to_set.input2_pre_scale_bias.pre_scale.push_back(tmp);
    }
    int idx = 0;
    for (auto entry : pre_shift2) {
        to_set.input2_pre_scale_bias.pre_scale[idx++].shift = entry;
    }
    for (auto entry : input_bias2) {
        mli::BiasType b;
        b.int_value = entry;
        to_set.input2_pre_scale_bias.input_bias.push_back(b);
    }
}

Conv2DAttrAdapter::Conv2DAttrAdapter(const mli::ConvAttributes& in)
    : groups(in.group) {
    int half_way = in.pads.size() / 2;
    for (int idx = 0; idx < half_way; idx++) pad_begin.push_back(in.pads[idx]);
    for (int idx = half_way; idx < half_way * 2; idx++)
        pad_end.push_back(in.pads[idx]);
    for (int32_t entry : in.strides) stride.push_back(entry);
    for (int32_t entry : in.dilations) dilation.push_back(entry);
}

Conv2DAttrAdapter::Conv2DAttrAdapter(const onnx::NodeProto& node_proto) {
    for (auto& attr_proto : node_proto.attribute()) {
        const std::string& attr_name = attr_proto.name();
        SET_ATTR_FROM_PROTO_ARRAY_IF(pad_begin, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pad_end, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(stride, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(dilation, ints)
        SET_ATTR_FROM_PROTO_SCAL_ELIF(groups, i)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(pad_value, ints)
        SET_ATTR_FROM_PROTO_ARRAY_ELIF(weights_zp, ints)
    }
}

void Conv2DAttrAdapter::to_conv_attrs(mli::ConvAttributes& to_set) {
    // FIXME: No idea how to set this for now, setting it to None for now.
    to_set.relu_type = mli::MLIReluType::None;
    to_set.group = groups;
    for (int32_t entry : dilation) to_set.dilations.push_back(entry);
    for (int32_t entry : stride) to_set.strides.push_back(entry);
    to_set.pads.reserve(pad_begin.size() * 2);
    std::copy(pad_begin.begin(), pad_begin.end(), to_set.pads.begin());
    std::copy(pad_end.begin(), pad_end.end(), to_set.pads.begin());
    // Not setting kernel_shape here because the back end will compute it from
    // the weights input.
}

}  // namespace snps_arc::metaware::nnac::backend
