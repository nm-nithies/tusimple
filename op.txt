# Copyright 2023-2024 Synopsys, Inc.
# This Synopsys software and all associated documentation are proprietary
# to Synopsys, Inc. and may only be used pursuant to the terms and conditions
# of a written license agreement with Synopsys, Inc.
# All other use, reproduction, modification, or distribution of the Synopsys
# software or the associated documentation is strictly prohibited.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import networkx as nx
import numpy as np

from nnac.core.log import Logger

logger = Logger("OPTIMIZATION")
"""
Fuse "Sub + Div" to a single "BatchNorm".
"""


def ONNXBatchNormalizationOp : ONNX_Op<"BatchNormalization", [Pure, 
    DeclareOpInterfaceMethods<ShapeInferenceOpInterface>,
    DeclareOpInterfaceMethods<ShapeHelperOpInterface>]> {
  
  let summary = "ONNX BatchNormalization operation";
  let description = [{
    This operator normalizes the input using a set of learnable parameters (scale, bias)
    and a set of statistics (mean, variance), with an added epsilon for numerical stability.
  }];
  
  let arguments = (ins 
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$X,      // Input tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$scale,  // Scale tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$B,      // Bias tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$mean,   // Mean tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$var,    // Variance tensor
    FAttr:$epsilonAttr                                    // Small epsilon value for stability
  );

  let results = (outs 
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$Y       // Output tensor
  );
  
  let extraClassDefinition = [{
    onnx_mlir::ONNXOpShapeHelper * ONNXBatchNormalizationOp::getShapeHelper(mlir::Operation *op, 
        mlir::ArrayRef<mlir::Value> oper, onnx_mlir::IndexExprBuilder *ieb, onnx_mlir::IndexExprScope *scope) {
      onnx_mlir::ONNXOpShapeHelper *sh = new onnx_mlir::ONNXBatchNormalizationOpShapeHelper(op, oper, ieb, scope);
      assert(sh && "failed to allocate shape helper");
      return sh;
    }
  }];
}


def get_constant_operand_index(opt, layer_name):
    # get the index of the constant operand to calculate scale and bias for BN node
    first_operand = opt.G.nodes[layer_name]["input"][0]
    second_operand = opt.G.nodes[layer_name]["input"][1]
    TensorDict = opt.TensorDict
    if first_operand in TensorDict.keys():
        return 0
    elif second_operand in TensorDict.keys():
        return 1
    else:
        return -1


def FuseSubAndDivIntoBN(opt):
    G = opt.G
    TensorDict = opt.TensorDict

    layers = list(nx.topological_sort(G))
    for layer in layers:
        if layer not in G.nodes:
            continue
        succs = list(G.successors(layer))
        if len(succs) > 0:
            sub_layer = list(G.successors(layer))[0]
            if G.nodes[sub_layer].get("op_type", None) == "Sub":
                # only support for 4D input
                if len(opt.ShapeDict[sub_layer]) == 4:
                    div_layer = list(G.successors(sub_layer))[0]
                    if G.nodes[div_layer].get("op_type", None) == "Div":
                        div_succ_layer = list(G.successors(div_layer))[0]
                        div_constant_operand_index = get_constant_operand_index(opt, div_layer)
                        sub_constant_operand_index = get_constant_operand_index(opt, sub_layer)
                        if sub_constant_operand_index == -1 or div_constant_operand_index == -1:
                            # apply pass only for Sub and Div with one constant operand
                            continue
                        div_value = TensorDict.get(G.nodes[div_layer]["input"][div_constant_operand_index], None)
                        sub_value = TensorDict.get(G.nodes[sub_layer]["input"][sub_constant_operand_index], None)

                        bn_name = layer.rsplit('/', 1)[0] + "_BatchNorm"
                        scale = np.array([(1/div_value)], dtype=np.float32)
                        bias = np.array([(-sub_value/div_value)], dtype=np.float32)
                        mean = np.array([0], dtype=np.float32)
                        var = np.array([1], dtype=np.float32)
                        scale_name = bn_name + "/scale"
                        bias_name = bn_name + "/bias"
                        mean_name = bn_name + "/mean"
                        var_name = bn_name + "/var"

                        TensorDict[scale_name] = scale
                        TensorDict[bias_name] = bias
                        TensorDict[mean_name] = mean
                        TensorDict[var_name] = var
                        bn_dict = {
                            "input": [layer, scale_name, bias_name, mean_name, var_name],
                            "output": [bn_name],
                            "op_type":  "BatchNormalization"
                        }

                        G.add_node(bn_name, **bn_dict)
                        G.add_edge(layer, bn_name)
                        G.add_edge(bn_name, div_succ_layer)
                        G.nodes[div_succ_layer]["input"][0] = bn_name
                        # remove Sub and Div layers
                        G.remove_node(sub_layer)
                        G.remove_node(div_layer)

                        opt.passes_counter["FuseSubAndDivIntoBN"] += 1



struct SubDivBatchNormFusePattern : public OpRewritePattern<ONNXSubOp> {
  using OpRewritePattern<ONNXSubOp>::OpRewritePattern;

  LogicalResult matchAndRewrite(
      ONNXSubOp subOp, PatternRewriter &rewriter) const final {
    // Match
    ONNXDivOp divOp;
    if (!isSubDivMatched(subOp, divOp)) // Check if the pattern matches.
      return failure();

    // Fetch input tensors for sub, div, and batch normalization.
    Value subInput1 = subOp.getOperand(0); // First input of Sub
    Value subInput2 = subOp.getOperand(1); // Second input of Sub
    Value divInput = divOp.getOperand(1);  // Div's denominator

    // Assuming the BatchNorm constants (scale, bias, mean, var) are known.
    // These can either be extracted from the surrounding context or passed in.
    Value scale = ...; // Placeholder: Value representing scale
    Value bias = ...;  // Placeholder: Value representing bias
    Value mean = ...;  // Placeholder: Value representing mean
    Value var = ...;   // Placeholder: Value representing variance
    FloatAttr epsilon = rewriter.getF32FloatAttr(1e-5f); // Small epsilon value

    // Rewrite
    Location loc = subOp.getLoc();
    onnx_mlir::MultiDialectBuilder<onnx_mlir::OnnxBuilder> create(rewriter, loc);

    // Create the fused BatchNorm operation with inputs, scale, bias, mean, and variance.
    auto batchNormResult = rewriter.create<ONNXBatchNormalizationOp>(
        loc, subInput1.getType(), subInput1, scale, bias, mean, var, epsilon);

    // Replace the original sub and div operations with the result of batch normalization.
    rewriter.replaceOp(subOp, batchNormResult.getResult(0)); // Replace subOp's result
    rewriter.replaceOp(divOp, batchNormResult.getResult(0)); // Replace divOp's result

    return success();
  }
};

Value createConstantTensor(PatternRewriter &rewriter, float value, Location loc) {
  auto constType = RankedTensorType::get({}, rewriter.getF32Type()); // Scalar
  auto constAttr = DenseElementsAttr::get(constType, value);
  return rewriter.create<ONNXConstantOp>(loc, constType, constAttr);
}






def ONNXBatchNormalizationOp : ONNX_Op<"BatchNormalization", [Pure, 
    DeclareOpInterfaceMethods<ShapeInferenceOpInterface>,
    DeclareOpInterfaceMethods<ShapeHelperOpInterface>]> {
  
  let summary = "ONNX BatchNormalization operation";
  let description = [{
    This operator normalizes the input using a set of learnable parameters (scale, bias)
    and a set of statistics (mean, variance), with an added epsilon for numerical stability.
  }];
  
  let arguments = (ins 
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$X,      // Input tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$scale,  // Scale tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$B,      // Bias tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$mean,   // Mean tensor
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$var,    // Variance tensor
    FAttr:$epsilonAttr                                    // Small epsilon value for stability
  );

  let results = (outs 
    AnyTypeOf<[TensorOf<[BF16]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>]>:$Y       // Output tensor
  );
  
  let extraClassDefinition = [{
    onnx_mlir::ONNXOpShapeHelper * ONNXBatchNormalizationOp::getShapeHelper(mlir::Operation *op, 
        mlir::ArrayRef<mlir::Value> oper, onnx_mlir::IndexExprBuilder *ieb, onnx_mlir::IndexExprScope *scope) {
      onnx_mlir::ONNXOpShapeHelper *sh = new onnx_mlir::ONNXBatchNormalizationOpShapeHelper(op, oper, ieb, scope);
      assert(sh && "failed to allocate shape helper");
      return sh;
    }
  }];
}


#include "mlir/IR/PatternMatch.h"
#include "mlir/Pass/Pass.h"
#include "src/Dialect/ONNX/ONNXOps.hpp"
#include "src/Dialect/ONNX/ONNXRewritePass.hpp"
#include "src/Dialect/ONNX/ONNXShapeHelper.hpp"

using namespace mlir;
using namespace onnx_mlir;

namespace {

struct FuseSubAndDivIntoBNPattern : public OpRewritePattern<ONNXSubOp> {
  using OpRewritePattern<ONNXSubOp>::OpRewritePattern;

  LogicalResult matchAndRewrite(ONNXSubOp subOp, PatternRewriter &rewriter) const final {
    // Check if the `Sub` operation is followed by a `Div` operation.
    Operation *subOutput = subOp.getOperation();
    if (!subOutput->hasOneUse()) return failure();

    Operation *divOp = *subOutput->user_begin();
    if (!isa<ONNXDivOp>(divOp)) return failure();

    // Ensure both `Sub` and `Div` have a constant operand.
    Value subInput0 = subOp.getOperand(0);
    Value subInput1 = subOp.getOperand(1);
    Value divInput1 = cast<ONNXDivOp>(divOp).getOperand(1);

    if (!subInput1.getDefiningOp<ONNXConstantOp>() || !divInput1.getDefiningOp<ONNXConstantOp>()) {
      return failure();
    }

    // Retrieve constant values for `Sub` and `Div`.
    auto subConstValue = subInput1.getDefiningOp<ONNXConstantOp>().value().cast<DenseElementsAttr>();
    auto divConstValue = divInput1.getDefiningOp<ONNXConstantOp>().value().cast<DenseElementsAttr>();

    float subValue = subConstValue.getValues<float>()[0];
    float divValue = divConstValue.getValues<float>()[0];

    // Create the `BatchNormalization` attributes.
    auto loc = subOp.getLoc();
    auto scaleAttr = DenseElementsAttr::get(rewriter.getF32TensorAttr(ArrayRef<float>({1.0f / divValue})));
    auto biasAttr = DenseElementsAttr::get(rewriter.getF32TensorAttr(ArrayRef<float>({-subValue / divValue})));
    auto meanAttr = DenseElementsAttr::get(rewriter.getF32TensorAttr(ArrayRef<float>({0.0f})));
    auto varAttr = DenseElementsAttr::get(rewriter.getF32TensorAttr(ArrayRef<float>({1.0f})));
    
    // Create the `BatchNormalization` op.
    Value scale = rewriter.create<ONNXConstantOp>(loc, scaleAttr);
    Value bias = rewriter.create<ONNXConstantOp>(loc, biasAttr);
    Value mean = rewriter.create<ONNXConstantOp>(loc, meanAttr);
    Value variance = rewriter.create<ONNXConstantOp>(loc, varAttr);

    SmallVector<Value, 5> bnInputs = {subOp.getOperand(0), scale, bias, mean, variance};

    // Use the same result type as `Div` for the new `BatchNormalization`.
    Type resultType = divOp->getResult(0).getType();
    Value bnOutput = rewriter.create<ONNXBatchNormalizationInferenceModeOp>(loc, resultType, bnInputs, /*epsilon=*/rewriter.getF64FloatAttr(1e-5), /*momentum=*/rewriter.getF64FloatAttr(0.9));

    // Replace the `Sub` and `Div` with the new `BatchNormalization` operation.
    rewriter.replaceOp(divOp, bnOutput);
    rewriter.eraseOp(subOp);

    return success();
  }
};

struct FuseSubAndDivIntoBNPass : public PassWrapper<FuseSubAndDivIntoBNPass, FunctionPass> {
  void runOnFunction() override {
    auto function = getFunction();

    // Apply the `FuseSubAndDivIntoBNPattern` to the function.
    OwningRewritePatternList patterns(&getContext());
    patterns.insert<FuseSubAndDivIntoBNPattern>(&getContext());
    applyPatternsAndFoldGreedily(function, std::move(patterns));
  }
};

} // end anonymous namespace

// Register the pass.
std::unique_ptr<Pass> createFuseSubAndDivIntoBNPass() {
  return std::make_unique<FuseSubAndDivIntoBNPass>();
}











def ReplaceSum(opt):
    G = opt.G
    layers = list(nx.topological_sort(G))
    for layer in layers:
        if G.nodes[layer].get("op_type", "") == "Sum":
            if len(G.nodes[layer]["input"]) != 2:
                continue
            G.nodes[layer]["op_type"] = "Add"

            opt.passes_counter["ReplaceSum"] += 1







# Copyright 2023-2024 Synopsys, Inc.
# This Synopsys software and all associated documentation are proprietary
# to Synopsys, Inc. and may only be used pursuant to the terms and conditions
# of a written license agreement with Synopsys, Inc.
# All other use, reproduction, modification, or distribution of the Synopsys
# software or the associated documentation is strictly prohibited.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
from math import isclose

import networkx as nx
from onnx import OperatorSetIdProto
import numpy as np

from nnac.core.customized_local_functions.onnx_local_functions import get_local_function_GELU
from nnac.core.log import Logger

from .single_layer_transforms import get_single_input_initializer

logger = Logger("OPTIMIZATION")


def FuseGeLU(opt):
    detect_gelu(opt)


def detect_gelu(opt):
    # custom_domain = ["ai.onnx.contrib", "snps.onnx.local"][1]
    G = opt.G

    layers = list(nx.topological_sort(G))  # [-1::-1]
    for layer in layers:
        if not G.has_node(layer):  # it's fused in previous loop
            continue
        succs = list(G.successors(layer))
        children = {"Mul": [], "Add": [], "Pow": [], "Div": []}
        for succ in succs:
            if "op_type" not in G.nodes[succ]:
                continue
            else:
                if G.nodes[succ]["op_type"] not in children:
                    continue
                children[G.nodes[succ]["op_type"]].append(succ)

        if (
            len(children["Mul"]) == 4 and len(children["Add"]) == 1 and len(succs) == 5
        ) or (
            len(children["Mul"]) == 1
            and len(children["Add"]) == 1
            and len(children["Pow"]) == 1
            and len(succs) == 3
        ):
            detect_tanh(opt, layer, children)
        elif (
            len(children["Mul"]) == 1 and len(children["Div"]) == 1 and len(succs) == 2
        ):
            detect_erf(opt, layer, children)


def detect_tanh(opt, layer, children):
    G = opt.G
    tensorDict = opt.TensorDict
    graph_attrs = opt.graph_attrs

    remove_node = []
    remove_tensor = []

    add_1 = children["Add"][0]
    add_pred_1 = find_non_root_pred(G, layer, add_1)
    if add_pred_1 is None or G.nodes[add_pred_1]["op_type"] != "Mul":
        logger.debug("[DEBUG] Can not find pred of (+0.44715*x^3)\n")
        return

    if G.nodes[add_pred_1]["op_type"] == "Mul" and len(children["Pow"]) == 1:
        add_pred_1_initializer = get_single_input_initializer(opt, add_pred_1)
        if (
            add_pred_1_initializer is None
            or not isclose(tensorDict[add_pred_1_initializer], 0.044715, abs_tol=1e-5)
        ):
            logger.debug("[DEBUG] The value of initializer is not equal to 0.044715\n")
            return
        remove_tensor.append(add_pred_1_initializer)
        remove_node.append(add_1)
        remove_node.append(add_pred_1)
    else:
        add_pred_2 = find_non_root_pred(G, layer, add_pred_1)
        if (
            add_pred_2 is None
            or G.nodes[add_pred_2]["op_type"] != "Mul"
        ):
            logger.debug("[DEBUG] Can not find pred of (+0.44715*x^3)\n")
            return
        add_pred_3 = find_non_root_pred(G, layer, add_pred_2)
        if (
            add_pred_3 is None
            or G.nodes[add_pred_3]["op_type"] != "Mul"
        ):
            logger.debug("[DEBUG] Can not find pred of (+0.44715*x^3)\n")
            return
        add_pred_1_initializer = get_single_input_initializer(opt, add_pred_1)
        add_pred_2_initializer = get_single_input_initializer(opt, add_pred_2)
        add_pred_3_initializer = get_single_input_initializer(opt, add_pred_3)
        initializer = None
        for i in [
            add_pred_1_initializer,
            add_pred_2_initializer,
            add_pred_3_initializer,
        ]:
            if i is not None:
                initializer = i
                break
        if (
            initializer is None
            or not isclose(tensorDict[initializer], 0.044715, abs_tol=1e-5)
        ):
            logger.debug(
                "[DEBUG] The value of initializer is not equal to 0.044715 or the initializer does not exist\n"
            )
            return
        remove_node.append(add_1)
        remove_node.append(add_pred_1)
        remove_node.append(add_pred_2)
        remove_node.append(add_pred_3)
        remove_tensor.append(initializer)
    # so far we have (x + 0.044715x^3)

    mul_1 = list(G.successors(add_1))
    if len(mul_1) != 1 or G.nodes[mul_1[0]]["op_type"] != "Mul":
        logger.debug("[DEBUG] Can not find Mul of math.sqrt(2./math.pi)\n")
        return
    mul_1 = mul_1[0]
    mul_1_initializer = get_single_input_initializer(opt, mul_1)
    if (
        mul_1_initializer is None
        or not isclose(tensorDict[mul_1_initializer], math.sqrt(2.0 / math.pi), abs_tol=1e-5)
    ):
        logger.debug(
            "[DEBUG] The value of initializer is not equal to math.sqrt(2./math.pi)"
            "or mul_1_initializer does not exist\n"
        )
        return
    remove_node.append(mul_1)
    remove_tensor.append(mul_1_initializer)
    # so far we have (math.sqrt(2./math.pi)*(x + 0.044715x^3))

    tanh = list(G.successors(mul_1))
    if len(tanh) != 1 or G.nodes[tanh[0]]["op_type"] != "Tanh":
        logger.debug("[DEBUG] Can not find Tanh\n")
        return
    tanh = tanh[0]
    remove_node.append(tanh)
    # so far we have tanh[math.sqrt(2./math.pi)*(x + 0.044715x^3))]

    add_2 = list(G.successors(tanh))
    if len(add_2) != 1 or G.nodes[add_2[0]]["op_type"] != "Add":
        logger.debug("[DEBUG] Can not find Add after Tanh\n")
        return
    add_2 = add_2[0]
    remove_node.append(add_2)
    # so far we have 1 + tanh[math.sqrt(2./math.pi)*(x + 0.044715x^3))]

    mul_2 = None
    mul_3 = None
    output_layer = None

    mul_2 = list(G.successors(add_2))
    if len(mul_2) != 1 or G.nodes[mul_2[0]]["op_type"] != "Mul":
        logger.debug("[DEBUG] Can not find Mul after add_2\n")
        return
    mul_2 = mul_2[0]
    mul_2_initializer = get_single_input_initializer(opt, mul_2)

    # mul_2 is the end of the GELU node, and one of the inputs of mul_2 is Mul with initializer value equals to 0.5
    if mul_2_initializer is None:
        mul_3 = find_non_root_pred(G, add_2, mul_2)
        if mul_3 is None or G.nodes[mul_3]["op_type"] != "Mul":
            logger.debug("[DEBUG] Can not find the Mul before last Mul\n")
            return
        mul_3_initializer = get_single_input_initializer(opt, mul_3)
        if (
            mul_3_initializer is None
            or not isclose(tensorDict[mul_3_initializer], 0.5, abs_tol=1e-5)
        ):
            return
        remove_tensor.append(mul_3_initializer)
        output_layer = mul_2
    # There must be a Mul after mul_2. The succeeding Mul is one of the succ node of root and is also the end of the
    # GELU node.
    else:
        if not isclose(tensorDict[mul_2_initializer], 0.5, abs_tol=1e-5):
            return
        mul_3 = list(G.successors(mul_2))
        if len(mul_3) != 1 or G.nodes[mul_3[0]]["op_type"] != "Mul":
            logger.debug("[DEBUG] Can not find the last Mul\n")
            return
        mul_3 = mul_3[0]
        output_layer = mul_3
        remove_tensor.append(mul_2_initializer)

    if mul_2 is None or mul_3 is None:
        logger.debug("[DEBUG] Can not find correct pattern in the end\n")
        return
    remove_node.append(mul_2)
    remove_node.append(mul_3)
    # so far we have 0.5x (1 + tanh[math.sqrt(2./math.pi)*(x + 0.044715x^3))])  Done!!

    # keep the output_layer, and replace its info as Gelu node
    remove_node.remove(output_layer)

    for node in remove_node:
        G.remove_node(node)
    # some graph reuses the initializer 0.044715, if we remove it, we can not found in other gelu pattern
    # for tensor in remove_tensor:
    #     del tensorDict[tensor]

    # add local function to insure the inference results are consistent with original graph
    custom_domain = ["ai.onnx.contrib", "snps.onnx.local"][1]

    # change node info of output_layer to info of Gelu
    G.nodes[output_layer]["op_type"] = "Gelu"
    G.nodes[output_layer]["input"] = [
        layer,
        "initializer_mul_1",
        "initializer_mul_2_1",
        "initializer_mul_2_4",
        "initializer_add_2",
    ]
    # G.nodes[output_layer]["domain"] = "com.microsoft"  # using onnxruntime kernel
    G.nodes[output_layer]["domain"] = custom_domain  # using local function

    G.add_edge(layer, output_layer)
    tensorDict["initializer_mul_1"] = np.array(0.5, dtype=np.float32)
    tensorDict["initializer_mul_2_1"] = np.array(0.044714998453855515, dtype=np.float32)
    tensorDict["initializer_mul_2_4"] = np.array(0.7978845834732056, dtype=np.float32)
    tensorDict["initializer_add_2"] = np.array([[[1]]], dtype=np.float32)

    cur_doms = [d.domain for d in graph_attrs["opset_import"]]
    if custom_domain not in cur_doms:
        graph_attrs["opset_import"].append(
            OperatorSetIdProto(version=1, domain=custom_domain)
        )
    if custom_domain == "snps.onnx.local":
        local_funcs = graph_attrs.get("functions", [])
        if "Gelu" not in [func.name for func in local_funcs]:
            func = get_local_function_GELU()
            local_funcs.append(func)
            graph_attrs["functions"] = local_funcs

    opt.passes_counter["FuseGeLU"] += 1


def detect_erf(opt, layer, children):
    G = opt.G
    tensorDict = opt.TensorDict
    graph_attrs = opt.graph_attrs

    div = children["Div"][0]
    erf = list(G.successors(div))
    if len(erf) != 1 or G.nodes[erf[0]]["op_type"] != "Erf":
        logger.debug("[DEBUG] Can not find Erf after Div\n")
        return
    erf = erf[0]
    add = list(G.successors(erf))
    if len(add) != 1 or G.nodes[add[0]]["op_type"] != "Add":
        logger.debug("[DEBUG] Can not find Add after Erf\n")
        return
    add = add[0]

    mul_add = list(G.successors(add))
    if len(mul_add) != 1 or G.nodes[mul_add[0]]["op_type"] != "Mul":
        logger.debug("[DEBUG] Can not find Mul after Add\n")
        return
    mul_add = mul_add[0]

    remove_node = [div, erf, add, mul_add]

    output_layer = None
    if children["Mul"][0] == mul_add:
        # format: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/test/testdata/transform/fusion/
        # gelu_format2_0.onnx
        #       x
        #     /   \
        #    |    Div
        #    |     |
        #    |    Erf
        #    |     |
        #    |    Add
        #     \   /
        #      Mul1
        #       |
        #      Mul2
        mul_end = list(G.successors(mul_add))
        if len(mul_end) != 1 or G.nodes[mul_end[0]]["op_type"] != "Mul":
            logger.debug("[DEBUG] Can not find Mul\n")
            return
        mul_end = mul_end[0]
        mul_initializer = get_single_input_initializer(opt, mul_end)
        if mul_initializer is None:
            mul_initializer = get_single_input_initializer(opt, mul_add)
        if not isclose(tensorDict[mul_initializer], 0.5, abs_tol=1e-5):
            logger.debug("[DEBUG] The value is not equal to 0.5\n")
            return
        remove_node.append(mul_end)
        output_layer = mul_end
    else:
        # format: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/test/testdata/transform/
        # fusion/gelu.onnx
        #       x
        #     /   \
        #    |    Div
        #    |     |
        #    |    Erf
        #    |     |
        #  Mul1   Add
        #     \   /
        #      Mul2
        mul_initializer = get_single_input_initializer(opt, children["Mul"][0])
        if mul_initializer is None:
            mul_initializer = get_single_input_initializer(opt, mul_add)
        if not isclose(tensorDict[mul_initializer], 0.5, abs_tol=1e-5):
            logger.debug("[DEBUG] The value is not equal to 0.5\n")
            return
        remove_node.append(children["Mul"][0])
        output_layer = mul_add

    # keep the output_layer, and replace its info as Gelu node
    remove_node.remove(output_layer)

    for node in remove_node:
        G.remove_node(node)

    # change node info of output_layer to info of Gelu
    G.nodes[output_layer]["op_type"] = "Gelu"
    G.nodes[output_layer]["input"] = [layer]
    G.nodes[output_layer]["domain"] = "com.microsoft"
    G.add_edge(layer, output_layer)

    cur_doms = [d.domain for d in graph_attrs["opset_import"]]
    if "com.microsoft" not in cur_doms:
        graph_attrs["opset_import"].append(
            OperatorSetIdProto(version=1, domain="com.microsoft")
        )

    opt.passes_counter["FuseGeLU"] += 1


def find_non_root_pred(G, root, layer):
    non_root_pred = None
    for pred in list(G.predecessors(layer)):
        if pred != root:
            non_root_pred = pred
            break
    return non_root_pred
