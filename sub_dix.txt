struct SubDivFuseBatchNormPattern : public OpRewritePattern<ONNXSubOp> {
  using OpRewritePattern<ONNXSubOp>::OpRewritePattern;

  LogicalResult matchAndRewrite(
      ONNXSubOp subOp, PatternRewriter &rewriter) const final {

    Location odsLoc = subOp.getLoc();
    onnx_mlir::MultiDialectBuilder<onnx_mlir::OnnxBuilder> create(
        rewriter, odsLoc);
    ONNXDivOp divOp;
    if (!isSubDivBatchNormMatched(subOp, divOp))
     { cout<<"643";
      return failure();}
    
    //   // Ensure both `Sub` and `Div` have a constant operand.
    // Value subInput0 = subOp.getOperand(0);
    // Value subInput1 = subOp.getOperand(1);
    // Value divInput1 = cast<ONNXDivOp>(divOp).getOperand(1);

    // if (!subInput1.getDefiningOp<ONNXConstantOp>() || !divInput1.getDefiningOp<ONNXConstantOp>()) {
    //   return failure();
    // }

    // // Retrieve constant values for `Sub` and `Div`.
    // auto subConstValue = subInput1.getDefiningOp<ONNXConstantOp>().value().cast<DenseElementsAttr>();
    // auto divConstValue = divInput1.getDefiningOp<ONNXConstantOp>().value().cast<DenseElementsAttr>();

    
    Value input = subOp.getOperand(0);
    // Value subConstValue = subOp.getOperand(1);
    // Value divConstValue = divOp.getOperand(1); 

    // FloatAttr a = mlir::dyn_cast<FloatAttr>(subConstValue);
    // cout<<getSingleFloatFromTensor(divConstValue);
    // auto constOp = subConstValue.getDefiningOp<mlir::arith::subOp>();
    // auto denseAttr = dyn_cast<mlir::DenseFPElementsAttr>(constOp.getValue());
    // float vals = *denseAttr.getValues<float>().begin();
    // cout<<vals<<"-----Float value ...";
    // cout<<"655655";
   
    float subValue = 0.5;  
    float divValue = 0.2;  
    Value scaleValue = createFloatTensorConstant(rewriter, subOp.getLoc(), 1.0 / divValue);
    Value biasValue = createFloatTensorConstant(rewriter, subOp.getLoc(),  -subValue / divValue);
    // Value temp = createFloatTensorConstant(rewriter, subOp.getLoc(), 1.0f);
    // Value scaleValue = create.onnx.div(temp, divConstValue);
    // Value biasValue = create.onnx.div(subConstValue, divConstValue);
    
    float mean = 0.0;
    float variance = 1.0f;
   
    FloatAttr epsilonAttr = rewriter.getF32FloatAttr(1e-5f); 
    FloatAttr momentumAttr =  rewriter.getF32FloatAttr(0.9);
    //  float b = momentumAttr.getValueAsDouble();
    // cout<<b;
   llvm::APFloat epsilon = epsilonAttr.getValue();
   llvm::APFloat momentum = momentumAttr.getValue();

  
  
    Value meanValue = createFloatTensorConstant(rewriter, subOp.getLoc(), mean);
    Value varValue = createFloatTensorConstant(rewriter, subOp.getLoc(), variance);
    cout<<"679679";
    
    SmallVector<Type, 3> outputTypes;
    outputTypes.emplace_back(input.getType());
    outputTypes.emplace_back(meanValue.getType());
    outputTypes.emplace_back(varValue.getType());
    

    auto bnOp = rewriter.create<ONNXBatchNormalizationOp>(
        subOp.getLoc(),outputTypes,input,
        scaleValue, biasValue, meanValue, varValue, epsilon, momentum);  // epsilon
        
    Value Y = bnOp.getY();
    Y.setType(divOp.getResult().getType());
    rewriter.replaceOp(divOp, Y);
    rewriter.eraseOp(subOp);
  
    // rewriter.replaceOp(divOp, bnOp.getResult(0));
    // rewriter.eraseOp(subOp);
    cout<<"689689";
    return success();


    //  Y.setType(instanceNormOp.getResult().getType());
    // // Replace operation.
    // rewriter.replaceOp(instanceNormOp, Y);
    // cout<<"SUCCeSS";
    // return success();
  }
};
