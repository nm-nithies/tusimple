The RecomposeLayerNormFromMulPattern pass detects and replaces ONNX Layer Normalization patterns with optimized LayerNorm or RMSLayerNorm operations, improving performance and reducing computation complexity.


The RecomposeGeluFromMulPattern pass rewrites Gelu activation functions represented as multiplications in ONNX by identifying exact or approximate forms. 
It replaces matched structures with a concise Gelu operation, maintaining the intended behavior of the computational graph.
he RecomposeGeluFromMulPattern pass rewrites GELU (Gaussian Error Linear Unit) operations by identifying ONNXMulOp patterns that match the GELU formula and replacing them with a direct ONNX GELU operation. This optimization simplifies the expression into a single ONNX operation for better execution efficiency.


This pass identifies a pattern of DequantizeLinear + MatMul + QuantizeLinear operations and rewrites it into a more efficient QLinearMatMul operation. 
It optimizes matrix multiplication in quantized neural networks by recomposing these operations into a single fused QLinearMatMul.

*****************************************************************************************************************************************************************************************

This pass rewrites the ONNXSoftmax operation by decomposing it into  ReduceMax, Subtract, Exp, ReduceSum, and Divide Operation. 
It optimizes the softmax computation by explicitly creating these intermediate steps, potentially enhancing performance or enabling further transformations.

This pass fuses Concat, Shape, and Transpose into a custom ONNXConcatShapeTransposeOp

This pass decomposes FusedMatMul into MatMul and Transpose operations with scaling by alpha

This pass converts ONNXInstanceNormalizationOp into LayerNorm, reshaping scale and bias for improved performance.

The SumToAddPattern rewrites ONNXSumOp into multiple ONNXAddOp operations, replacing input summation with individual additions.

The ReplaceCastLikeByCastPattern transforms ONNXCastLikeOp into an appropriate cast operation based on the target type, preserving the output type. It handles both ranked and unranked tensor types, ensuring the output is correctly cast to match the specified target type.

This pass replaces a 1x1 convolution with a matrix multiplication, reshaping inputs and outputs for efficiency. Bias is handled if present.

The provided C++ code parses the Einsum equation and computes the axes for summation and contraction to facilitate the decomposition of the Einsum operation into MatMul and ReduceSum operations.



Pattern 1 transforms ONNXGroupNormalizationOp into LayerNorm.
Pattern 2 transforms ONNXGroupNormalizationV18Op into LayerNorm.
