The C++ code rewrites ONNXSumOp during MLIR transformation, handling sums with multiple inputs. The Python code replaces Sum with Add operations in a computational graph only when there are exactly two inputs. Although both transform Sum to Add, they operate in different contexts and have different input constraints.

SumToAddPattern
ReplaceSum

4. Pattern Matching
C++ Pass:

The C++ pass uses low-level pattern matching via functions like operandOfOpDefinedBy<ONNXDivOp> to traverse and identify the desired sequence of operations in the IR.
The C++ pass can finely control each operation, ensuring that operations like Mul, Div, Sqrt, and others are matched precisely.
Python Pass:

A Python pass would use a higher-level pattern matching approach. For example, it might use graph traversal techniques to inspect the nodes in the computational graph, looking for patterns like ReduceMean, Sub, Mul, and Sqrt.
Pattern matching in Python is often easier to express due to dynamic typing and flexibility, but it may not be as performant or as precise as the C++ version.
5. Rewrite and Replacement Mechanism
C++ Pass:

Uses PatternRewriter for efficient replacement of operations in the IR, with low-level control over the newly created operations (onnx.layerNorm or onnx.RMSLayerNorm).
The MultiDialectBuilder<OnnxBuilder> helps construct new ONNX operations, making the rewriting process efficient and integrated with MLIR.
Python Pass:

The rewriting mechanism in Python would likely involve modifying the nodes in the graph directly and replacing matched subgraphs with new subgraphs.
The abstraction level is higher, and the performance impact could be more significant depending on the graph manipulation library used.


RecomposeLayerNormFromMulPattern
FusePrimitivesIntoLayerNorm

***********************************

Detecting Tanh-based GeLU: Identifies patterns like 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))) through a combination of operations (Mul, Add, Pow, Tanh). The detected subgraph is replaced with a single GeLU node.

Detecting Erf-based GeLU: Finds patterns like 0.5 * x * (1 + erf(x / sqrt(2))) using operations like Mul, Div, Erf, Add, and replaces them with a GeLU node.

This pattern detects subgraphs matching the exact (involving Mul, Add, Erf, and Div) or approximate (involving Mul, Add, Tanh, Pow) GeLU formulas and replaces them with a single ONNX GeLU operation. It optimizes the graph by collapsing these multiple operations into a single, efficient GeLU node.


*********************************

**************************
This pass decomposes the ONNXEinsumOp into simpler ONNX operations, enabling more efficient execution on supported backends. By breaking down complex Einstein summation expressions, the pass transforms them into fundamental operations such as ReduceSum, MatMul, Reshape, and Transpose. This decomposition facilitates better optimization and hardware compatibility during model compilation.

The code optimizes computational graphs by replacing Einsum operations with equivalent MatMul operations when possible. It employs two patterns: the first handles direct substitutions with potential transpositions, while the second supports more complex scenarios involving reshaping. The implementation also includes logging for debugging and tracking the number of transformations applied.

**************************

****************

The pass looks for a specific set of ONNX operations (e.g., ReduceMean, Mul, Div, Sqrt, Add) that together implement LN or RMS LN.
It checks if the operations are used in the correct sequence to form either a full Layer Normalization pattern or an RMS Layer Normalization pattern.

This code provides an optimization pass designed to fuse primitive operators into a LayerNormalization operation within an ONNX graph. The process revolves around finding patterns of operations that represent layer normalization, such as ReduceMean, Sub, Mul, Add, and Pow, and then combining them into a single LayerNormalization node to optimize the graph.
