In file included from /workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/PatternMatch.h:12,
                 from ../src/Dialect/ONNX/Transforms/temp.cpp:24:
/workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/Builders.h: In instantiation of 'OpTy mlir::OpBuilder::create(mlir::Location, Args&& ...) [with OpTy = mlir::ONNXBatchNormalizationInferenceModeOp; Args = {mlir::Type, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::FloatAttr&}]':
../src/Dialect/ONNX/Transforms/temp.cpp:643:77:   required from here
/workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/Builders.h:513:16: error: no matching function for call to 'mlir::ONNXBatchNormalizationInferenceModeOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::Type, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::FloatAttr&)'
  513 |     OpTy::build(*this, state, std::forward<Args>(args)...);
      |     ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ../src/Dialect/ONNX/ONNXOps.hpp:33,
                 from ../src/Dialect/ONNX/Transforms/temp.cpp:30:
./src/Dialect/ONNX/ONNXOps.hpp.inc:3692:15: note: candidate: 'static void mlir::ONNXBatchNormalizationInferenceModeOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::Type, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::FloatAttr, mlir::FloatAttr)'
 3692 |   static void build(::mlir::OpBuilder &odsBuilder, ::mlir::OperationState &odsState, ::mlir::Type o_Y, ::mlir::Value X, ::mlir::Value scale, ::mlir::Value B, ::mlir::Value mean, ::mlir::Value var, ::mlir::FloatAttr epsilon, ::mlir::FloatAttr momentum);



 auto batchNormResult = rewriter.create<ONNXBatchNormalizationInferenceModeOp>(
        loc, subInput1.getType(), subInput1, scale, bias, mean, var, epsilon);



def ONNXBatchNormalizationOp:ONNX_Op<"BatchNormalization",
  [Pure, DeclareOpInterfaceMethods<ShapeInferenceOpInterface>, DeclareOpInterfaceMethods<ShapeHelperOpInterface>]> {
  let summary = "ONNX BatchNormalization operation";
  let description = [{
  Carries out batch normalization as described in the paper
  https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
  There are five required inputs 'X', 'scale', 'B', 'input_mean' and
  'input_var'.
  Note that 'input_mean' and 'input_var' are expected to be the estimated
  statistics in inference mode (training_mode=False, default),
  and the running statistics in training mode (training_mode=True).
  There are multiple cases for the number of outputs, which we list below:
  
  * Output case #1: Y, running_mean, running_var (training_mode=True)
  * Output case #2: Y (training_mode=False)
  
  When training_mode=False, extra outputs are invalid.
  The outputs are updated as follows when training_mode=True:
  ```
  running_mean = input_mean * momentum + current_mean * (1 - momentum)
  running_var = input_var * momentum + current_var * (1 - momentum)
  
  Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B
  ```
  where:
  ```
  current_mean = ReduceMean(X, axis=all_except_channel_index)
  current_var =  ReduceVar(X, axis=all_except_channel_index)
  ```
  Notice that `ReduceVar` refers to the population variance, and it equals to
  `sum(sqrd(x_i - x_avg)) / N`
  where `N` is the population size (this formula does not use sample size `N - 1`).
  
  The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.
  
  When training_mode=False:
  ```
  Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B
  ```
  
  For previous (depreciated) non-spatial cases, implementors are suggested
  to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.
  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
  }];
  let arguments = (ins AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>]>:$X,
    AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>]>:$scale,
    AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>]>:$B,
    AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>]>:$input_mean,
    AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>]>:$input_var,
    DefaultValuedAttr<F32Attr, "1e-05">:$epsilon,
    DefaultValuedAttr<F32Attr, "0.9">:$momentum,
    DefaultValuedAttr<SI64Attr, "0">:$training_mode);
  let results = (outs AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>]>:$Y,
    AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, NoneType]>:$running_mean,
    AnyTypeOf<[TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, NoneType]>:$running_var);
  let extraClassDeclaration = [{
    static int getNumberOfOperands() {
      return 5;
    }
    static int getNumberOfResults() {
      return 3;
    }
    static std::vector<int> getTypeMap() {
      return {30,33,33};
    }
  }];





"""
    Calculate the accuracy of predicted line values compared to ground truth values within a given threshold.

    The function replaces negative values in both predicted and ground truth arrays with -100 and calculates
    the fraction of predictions that are within the specified threshold compared to the ground truth.

    Returns the accuracy as a fraction of the total number of ground truth values.
    """


   """
    Calculate the angle (in radians) of a line fitted to given points in a 2D space (xs, y_samples).
    
    The function filters out invalid points (where xs is negative), fits a linear regression to the remaining points,
    and calculates the angle of the line using the slope of the fitted line.

    :param xs: Array of x-coordinates (can contain negative values).
    :param y_samples: Corresponding array of y-coordinates.
    :return: Angle theta in radians.
    """



struct SubDivFuseBatchNormPattern : public OpRewritePattern<ONNXSubOp> {
  using OpRewritePattern<ONNXSubOp>::OpRewritePattern;

  LogicalResult matchAndRewrite(
      ONNXSubOp subOp, PatternRewriter &rewriter) const final {
   
    ONNXDivOp divOp;
    if (!isSubDivBatchNormMatched(subOp, divOp))
     { cout<<"643";
      return failure();}
    
   
    Value input = subOp.getOperand(0);
    Type resType = subOp.getResult().getType();
   
    // Value subConstValue = subOp.getOperand(1);  
    // Value divConstValue = divOp.getOperand(1);  
    //  cout<<"650650";
    
    // auto subConstOp = subConstValue.getDefiningOp<ONNXConstantOp>();
    // auto divConstOp = divConstValue.getDefiningOp<ONNXConstantOp>();
   
    cout<<"655655";
   
    float subValue = 0.5;  
    float divValue = 0.2;  
   
    float scale = 1.0f / divValue;
    float bias = -subValue / divValue;
    float mean = 0.0f;
    float variance = 1.0f;
    FloatAttr epsilon = rewriter.getF32FloatAttr(1e-5f); 
    FloatAttr momentum =  rewriter.getF32FloatAttr(0.9);


  
    Value scaleValue = createFloatConstant(rewriter, subOp.getLoc(), scale);
    Value biasValue = createFloatConstant(rewriter, subOp.getLoc(), bias);
    Value meanValue = createFloatConstant(rewriter, subOp.getLoc(), mean);
    Value varValue = createFloatConstant(rewriter, subOp.getLoc(), variance);
     cout<<"679679";
    
    SmallVector<Type, 2> outputTypes;
    outputTypes.emplace_back(subOp.getResult().getType());
    outputTypes.emplace_back(divOp.getResult().getType());
    

    auto bnOp = rewriter.create<ONNXBatchNormalizationOp>(
        subOp.getLoc(),input.getType(),input,
        scaleValue, biasValue, meanValue, varValue, epsilon, momentum);  // epsilon

    // Replace the Div operation with the result of BatchNorm
    rewriter.replaceOp(divOp, bnOp.getResult());
    // Remove the Sub operation
    rewriter.eraseOp(subOp);
    cout<<"689689";
    return success();
  }
};

src/Dialect/ONNX/Transforms/temp.cpp
../src/Dialect/ONNX/Transforms/temp.cpp: In member function 'virtual llvm::LogicalResult {anonymous}::SubDivFuseBatchNormPattern::matchAndRewrite(mlir::ONNXSubOp, mlir::PatternRewriter&) const':
../src/Dialect/ONNX/Transforms/temp.cpp:690:46: error: no matching function for call to 'mlir::ONNXBatchNormalizationOp::getResult()'
  690 |     rewriter.replaceOp(divOp, bnOp.getResult());
      |                                              ^
In file included from /workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/Matchers.h:20,
                 from ../src/Dialect/ONNX/Transforms/temp.cpp:23:
/workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/OpDefinition.h:630:9: note: candidate: 'mlir::Value mlir::OpTrait::detail::MultiResultTraitBase<ConcreteType, TraitType>::getResult(unsigned int) [with ConcreteType = mlir::ONNXBatchNormalizationOp; TraitType = mlir::OpTrait::NResults<3>::Impl]'
  630 |   Value getResult(unsigned i) { return this->getOperation()->getResult(i); }
      |         ^~~~~~~~~
/workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/OpDefinition.h:630:9: note:   candidate expects 1 argument, 0 provided
In file included from /workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/PatternMatch.h:12,
                 from ../src/Dialect/ONNX/Transforms/temp.cpp:24:
/workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/Builders.h: In instantiation of 'OpTy mlir::OpBuilder::create(mlir::Location, Args&& ...) [with OpTy = mlir::ONNXBatchNormalizationOp; Args = {mlir::Type, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::FloatAttr&, mlir::FloatAttr&}]':
../src/Dialect/ONNX/Transforms/temp.cpp:687:70:   required from here
/workspace/ONNX_MLIR/llvm-project/mlir/include/mlir/IR/Builders.h:513:16: error: no matching function for call to 'mlir::ONNXBatchNormalizationOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::Type, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::Value&, mlir::FloatAttr&, mlir::FloatAttr&)'
  513 |     OpTy::build(*this, state, std::forward<Args>(args)...);
      |     ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ../src/Dialect/ONNX/ONNXOps.hpp:33,
                 from ../src/Dialect/ONNX/Transforms/temp.cpp:30:
./src/Dialect/ONNX/ONNXOps.hpp.inc:3966:15: note: candidate: 'static void mlir::ONNXBatchNormalizationOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::Type, mlir::Type, mlir::Type, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::FloatAttr, mlir::FloatAttr, mlir::IntegerAttr)'
 3966 |   static void build(::mlir::OpBuilder &odsBuilder, ::mlir::OperationState &odsState, ::mlir::Type Y, ::mlir::Type running_mean, ::mlir::Type running_var, ::mlir::Value X, ::mlir::Value scale, ::mlir::Value B, ::mlir::Value input_mean, ::mlir::Value input_var, ::mlir::FloatAttr epsilon, ::mlir::FloatAttr momentum, ::mlir::IntegerAttr training_mode);
      |               ^~~~~
./src/Dialect/ONNX/ONNXOps.hpp.inc:3966:15: note:   candidate expects 13 arguments, 10 provided
./src/Dialect/ONNX/ONNXOps.hpp.inc:3967:15: note: candidate: 'static void mlir::ONNXBatchNormalizationOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::TypeRange, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::FloatAttr, mlir::FloatAttr, mlir::IntegerAttr)'
 3967 |   static void build(::mlir::OpBuilder &odsBuilder, ::mlir::OperationState &odsState, ::mlir::TypeRange resultTypes, ::mlir::Value X, ::mlir::Value scale, ::mlir::Value B, ::mlir::Value input_mean, ::mlir::Value input_var, ::mlir::FloatAttr epsilon, ::mlir::FloatAttr momentum, ::mlir::IntegerAttr training_mode);
      |               ^~~~~
./src/Dialect/ONNX/ONNXOps.hpp.inc:3967:15: note:   candidate expects 11 arguments, 10 provided
./src/Dialect/ONNX/ONNXOps.hpp.inc:3968:15: note: candidate: 'static void mlir::ONNXBatchNormalizationOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::Type, mlir::Type, mlir::Type, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::Value, llvm::APFloat, llvm::APFloat, int64_t)'
 3968 |   static void build(::mlir::OpBuilder &odsBuilder, ::mlir::OperationState &odsState, ::mlir::Type Y, ::mlir::Type running_mean, ::mlir::Type running_var, ::mlir::Value X, ::mlir::Value scale, ::mlir::Value B, ::mlir::Value input_mean, ::mlir::Value input_var, ::llvm::APFloat epsilon, ::llvm::APFloat momentum, int64_t training_mode = 0);
      |               ^~~~~
./src/Dialect/ONNX/ONNXOps.hpp.inc:3968:15: note:   candidate expects 13 arguments, 10 provided
./src/Dialect/ONNX/ONNXOps.hpp.inc:3969:15: note: candidate: 'static void mlir::ONNXBatchNormalizationOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::TypeRange, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::Value, llvm::APFloat, llvm::APFloat, int64_t)'
 3969 |   static void build(::mlir::OpBuilder &odsBuilder, ::mlir::OperationState &odsState, ::mlir::TypeRange resultTypes, ::mlir::Value X, ::mlir::Value scale, ::mlir::Value B, ::mlir::Value input_mean, ::mlir::Value input_var, ::llvm::APFloat epsilon, ::llvm::APFloat momentum, int64_t training_mode = 0);
      |               ^~~~~
./src/Dialect/ONNX/ONNXOps.hpp.inc:3969:239: note:   no known conversion for argument 9 from 'mlir::FloatAttr' to 'llvm::APFloat'








../src/Dialect/ONNX/Transforms/temp.cpp:669:53: error: conversion from 'mlir::FloatAttr' to non-scalar type 'llvm::APFloat' requested
  669 |     llvm::APFloat epsilon = rewriter.getF32FloatAttr(1e-5f);
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~
../src/Dialect/ONNX/Transforms/temp.cpp:670:55: error: conversion from 'mlir::FloatAttr' to non-scalar type 'llvm::APFloat' requested
  670 |     llvm::APFloat momentum =  rewriter.getF32FloatAttr(0.9);
      |                               ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~
ninja: build stopped: subcommand failed.



static void mlir::ONNXBatchNormalizationOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::TypeRange, mlir::Value, mlir::Value, mlir::Value, mlir::Value, mlir::Value, llvm::APFloat, llvm::APFloat, int64_t): Assertion `resultTypes.size() == 3u && "mismatched number of results"' failed.






loc("/Sub"): error: 'onnx.BatchNormalization' op operand #1 must be tensor of 16-bit float values or tensor of 32-bit float values or tensor of 64-bit float values or tensor of bfloat16 type values, but got 'f32'
loc("/Sub"): error: 'onnx.BatchNormalization' op verification failed
loc("/Sub"): error: 'onnx.BatchNormalization' op operand #1 must be tensor of 16-bit float values or tensor of 32-bit float values or tensor of 64-bit float values or tensor of bfloat16 type values, but got 'f32'
loc("/Sub"): error: 'onnx.BatchNormalization' op verification failed
loc("/Sub"): error: 'onnx.BatchNormalization' op operand #1 must be tensor of 16-bit float values or tensor of 32-bit float values or tensor of 64-bit float values or tensor of bfloat16 type values, but got 'f32'




import torch
import torch.nn as nn
import torch.onnx

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x, y):
        # Add operatio
        
        # Subtract with constant 0.5
        
        sub_result = x - y 
        
        # Divide with constant 0.2
        output = sub_result / y
        
        # Reshape operation
       
        return output

# Create a model instance
model = SimpleModel()

# Dummy input tensors
x = torch.randn(1, 3, 28, 28)
y = torch.randn(1, 3, 28, 28)

# Perform a forward pass to test the model
output = model(x, y)
print("Model output:", output)

# Export the model to ONNX format
torch.onnx.export(
    model,                        # Model being run
    (x, y),                       # Model input (dummy inputs for export)
    "simple_model_with_constants.onnx",  # File name to export the model
    export_params=True,           # Store trained parameters
    opset_version=12,             # ONNX opset version
    do_constant_folding=True,     # Fold constant values for optimization
    input_names=['input_x', 'input_y'],  # Input names
    output_names=['output'],      # Output names
    dynamic_axes={'input_x': {0: 'batch_size'},  # Dynamic axes for input
                  'input_y': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}   # Dynamic axes for output
)

print("Model exported to simple_model_with_constants.onnx")



mlir::Value createFloatConstant(mlir::OpBuilder &builder, mlir::Location loc, float value) {
    auto floatType = builder.getF32Type();
    mlir::FloatAttr floatAttr = builder.getF32FloatAttr(value);
    return builder.create<mlir::arith::ConstantOp>(loc, floatType, floatAttr).getResult();
}




mlir::Value createFloatTensorConstant(mlir::OpBuilder &builder, mlir::Location loc, float value) {
    // Define the element type as f32 (float).
    auto elementType = builder.getF32Type();
    
    // Create a RankedTensorType for a 1-element tensor.
    auto tensorType = mlir::RankedTensorType::get({}, elementType);  // Empty array means scalar tensor.
    
    // Create a FloatAttr for the constant value.
    mlir::FloatAttr floatAttr = builder.getF32FloatAttr(value);
    
    // Create the constant tensor operation using arith::ConstantOp.
    return builder.create<mlir::arith::ConstantOp>(loc, tensorType, floatAttr).getResult();
}



loc("/Sub"): error: 'arith.constant' op failed to verify that all of {value, result} have same type
